{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.engine.topology import Layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, TimeDistributed, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "import t3f\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train / 127.5 - 1.0\n",
    "x_test = x_test / 127.5 - 1.0\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Flatten(), input_shape=(28, 28)))\n",
    "model.add(LSTM(25))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 25)                5400      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 5,660\n",
      "Trainable params: 5,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=1e-3)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 75s - loss: 0.8287 - acc: 0.7345 - val_loss: 0.3520 - val_acc: 0.8934\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 85s - loss: 0.2618 - acc: 0.9242 - val_loss: 0.2048 - val_acc: 0.9398\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 74s - loss: 0.1820 - acc: 0.9462 - val_loss: 0.1624 - val_acc: 0.9512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffa7e82ae48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TT LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In comparison to 'Tensor-Train Recurrent Neural Networks for Video Classification' paper we may compress both kernel and recurrent kernel in our recurrent cells. If we use basic implementaton from paper we will have much more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lstm import TT_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100)\n",
      "(?, 25)\n",
      "A TT-Matrix variable of size 25 x 100, underlying tensor shape: (5, 5) x (20, 5), TT-ranks: (1, 4, 1)\n",
      "(?, 100)\n",
      "(?, 100)\n",
      "(?, 25)\n",
      "A TT-Matrix variable of size 25 x 100, underlying tensor shape: (5, 5) x (20, 5), TT-ranks: (1, 4, 1)\n",
      "(?, 100)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Flatten(), input_shape=(28, 28)))\n",
    "model.add(TT_LSTM(row_dims=[4, 7], column_dims=[5, 5], tt_rank=4))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_2 (TimeDist (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "tt_lstm_1 (TT_LSTM)          (None, 25)                960       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,220\n",
      "Trainable params: 1,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=1e-2)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 105s - loss: 0.6528 - acc: 0.7847 - val_loss: 0.2874 - val_acc: 0.9144\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 99s - loss: 0.2851 - acc: 0.9141 - val_loss: 0.2279 - val_acc: 0.9307\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 108s - loss: 0.2484 - acc: 0.9259 - val_loss: 0.2013 - val_acc: 0.9407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffa7f0ccdd8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TTLSTM gives us similar to basic LSTM with 5x less params (just for toy example, in real cases ew may have 100x and more compression rates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TTLSTM + YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using such layers allows us to train recurrent detectors end 2 end (in comparison to https://arxiv.org/abs/1607.04648 ) their they use precomputed features from convolutional networks.\n",
    "Furthermore, even if we use pretrained features we will have ~200 millions of parameters in lstm if have default detection networks like yolo with LSTM layer. With TTLSTM we have several dozens of thousands of additional weights.\n",
    "\n",
    "If we use implementation from TTRNN for classification paper we will have ~100 millions parameters due non compression of internal recurrent kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
